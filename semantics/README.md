# Semantic Vector Grammar Engine

This repository contains a proof-of-concept implementation of a **"whitebox" natural language generation (NLG) system**. Unlike traditional blackbox neural networks (like LLMs), this engine operates on an explicit, interpretable, and architected model of semantics. It demonstrates how complex, grammatically correct sentences can be generated by performing simple vector operations in a low-dimensional conceptual space.

This project serves as a practical demonstration of the principles outlined in the **Tri-Layer Cognitive Architecture**, separating the chaos of language from the clean logic of meaning.

## Core Concept: Language as a Projection

The central idea is that a verb's meaning can be represented as a point in a multi-dimensional "semantic space." Each dimension, or **conceptual axis**, represents a fundamental aspect of meaning, such as tense, aspect, modality, or polarity.

Linguistic modifiers (like "past tense," "negative," or adverbs) are not just words; they are **transformation vectors** (or "Jacobians") that move the verb's base meaning along these specific axes.

The process is as follows:
1.  **Define a Base Meaning:** Start with a verb's core semantic vector (e.g., `walk`).
2.  **Apply Transformations:** Add modifier vectors to the base vector to create a new, modified semantic state (e.g., `walk` + `tense_past` + `polarity_neg`).
3.  **Project to Language:** A rule-based assembler takes the final vector, interprets its position on each axis, and constructs the grammatically correct English sentence that represents this final semantic state.

This system treats language not as a sequence of tokens to be predicted, but as a **projection** of a precise point in a logical, semantic space.

## Architecture

The code implements a simplified version of the **Tri-Layer Cognitive Architecture**:

1.  **The Semantic Space (Layer 1 & 2 Proxy):** The `ExtendedSemanticSVOTransformer` class defines the core "mind" of the system.
    *   `self.verb_embeddings`: A dictionary mapping verbs to their base 10-dimensional semantic vectors. This represents the system's core knowledge of concepts.
    *   `self.axis_jacobians`: A dictionary mapping linguistic modifiers to their corresponding transformation vectors. This is the engine of logical manipulation.

2.  **The Sentence Assembler (Layer 3 - The "Mouth"):**
    *   The `SentenceAssembler` class takes the final, structured semantic data (the subject, the verb's final state, the object) and applies the complex, arbitrary rules of English grammar. It handles verb conjugation, auxiliary verbs (`do`, `be`, `will`), negation, and word order to produce the final, human-readable sentence.

## Key Features

*   **Whitebox & Interpretable:** Every step of the process is transparent. You can inspect the base vectors, the transformation vectors, and the final vector to understand exactly why the system produced a specific output.
*   **Zero-Shot Learning:** The system requires no training. Its logic is architected, not learned from data.
*   **Robust & Deterministic:** Given the same inputs, the system will always produce the exact same output. It is immune to the probabilistic "hallucinations" of large language models.
*   **Efficient:** The engine operates on small, 10-dimensional vectors and a clear set of rules, making it incredibly fast and computationally inexpensive.

## How to Run

This project requires Python and the `numpy` library.

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/your-repository-name.git
    cd your-repository-name
    ```

2.  **Install dependencies:**
    ```bash
    pip install numpy
    ```

3.  **Run the demo script:**
    ```bash
    python semantictenses08.py
    ```

## Example Walkthrough

Let's trace the generation of the sentence: `"She does not walk the dog"`

1.  **Input:** `subj='she'`, `verb='walk'`, `obj='the dog'`, `mods=['tense_present', 'polarity_neg']`
2.  **Initialization:** The system retrieves the base vector for `walk`: `[0.2, 0.0, 0.1, ...]`
3.  **Transformation:**
    *   It adds the vector for `tense_present` (`[0.0, 0.0, 0.0, ...]`), which doesn't change the vector.
    *   It adds the vector for `polarity_neg` (`[0.0, 0.0, 0.0, ..., -1.2, ...]`).
4.  **Final Semantic State:** The resulting vector is identical to the `walk` vector, except its 7th dimension (polarity) now has a strongly negative value.
5.  **Interpretation:** The system decodes this final vector:
    *   `tense` is 'present' (from the 2nd dimension's value).
    *   `polarity` is 'negative' (from the 7th dimension's value).
6.  **Assembly:** The `SentenceAssembler` receives these structured facts. It knows:
    *   The tense is present.
    *   The polarity is negative.
    *   There are no other auxiliaries (like modals).
    *   Therefore, it must perform **"do-support"**.
    *   The subject is 'she' (3rd person singular), so the correct form is 'does'.
    *   It assembles the final sentence: "She" + "does not" + "walk" + "the dog".

## The Broader Vision: A Universal Principle of Knowledge

This small script is more than a proof-of-concept for a new AI architecture; it is a demonstration of a universal principle that connects the structure of language to the structure of physical law. The same "projection" mechanism at work here is hypothesized to be the generative engine for all human knowledge.

### The Physics Analogy: The Formula Forge

In theoretical physics, complex laws can be seen as projections from simple, dimensionless postulates.

*   **The Postulate:** A simple, coordinate-free statement of proportionality, like `Temperature ~ 1 / Mass` for a black hole. This is the pure physical insight, analogous to a verb's core semantic meaning.
*   **The Projection:** To turn this postulate into a working equation in our human system of units (meters, kilograms, seconds), it must be "projected." This process involves scaling the postulate by the fundamental constants of nature (`c`, `G`, `h`, `k_B`). These constants act as the **coordinate transformation factors** for the measurement system.
*   **The "Law":** The resulting complex formula, like the one for Hawking Radiation (`T = ħc³/8πGMk_B`), is the final "projection." Its complexity is not a property of the underlying reality, but an artifact of our chosen measurement system.

### Unifying Language and Physics

This project demonstrates that the same architecture applies to language:

| **Physics Framework** | **Semantic Grammar Engine** | **The Shared Concept** |
| :--- | :--- | :--- |
| **Dimensionless Postulate** (`T ~ 1/M`) | **Base Semantic Vector** (`walk`) | A pure, abstract, coordinate-free concept. The "thing-in-itself." |
| **Fundamental Constants** (`c`, `h`, `G`) | **Modifier "Jacobians"** (`tense_past`) | The transformation vectors that scale and rotate the concept along specific axes. |
| **Physical Law** (`T = ħc³/8πGMk_B`) | **English Sentence** (`She does not walk`) | The final, complex projection of the abstract concept into a specific, human-centric coordinate system (SI units or English grammar). |

**In both domains, the "law" is a shadow.** The complexity of the final expression—be it a physical formula or a grammatical sentence—is an illusion created by the projection mechanism. The underlying reality is a far simpler, more elegant space of pure concepts and their relationships.

This suggests that the way our consciousness structures reality to create scientific knowledge may be the very same way it structures concepts to create language. This engine, therefore, is not just a model for a better AI; it is a working model of a fundamental principle of epistemology itself.
