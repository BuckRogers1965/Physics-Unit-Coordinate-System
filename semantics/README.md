# Semantic Vector Grammar Engine

This repository contains a proof-of-concept implementation of a **"whitebox" natural language generation (NLG) system**. Unlike traditional blackbox neural networks (like LLMs), this engine operates on an explicit, interpretable, and architected model of semantics. It demonstrates how complex, grammatically correct sentences can be generated by performing simple vector operations in a low-dimensional conceptual space.

This project serves as a practical demonstration of the principles outlined in the **Tri-Layer Cognitive Architecture**, separating the chaos of language from the clean logic of meaning.

## Core Concept: Language as a Projection

The central idea is that a verb's meaning can be represented as a point in a multi-dimensional "semantic space." Each dimension, or **conceptual axis**, represents a fundamental aspect of meaning, such as tense, aspect, modality, or polarity.

Linguistic modifiers (like "past tense," "negative," or adverbs) are not just words; they are **transformation vectors** (or "Jacobians") that move the verb's base meaning along these specific axes.

The process is as follows:
1.  **Define a Base Meaning:** Start with a verb's core semantic vector (e.g., `walk`).
2.  **Apply Transformations:** Add modifier vectors to the base vector to create a new, modified semantic state (e.g., `walk` + `tense_past` + `polarity_neg`).
3.  **Project to Language:** A rule-based assembler takes the final vector, interprets its position on each axis, and constructs the grammatically correct English sentence that represents this final semantic state.

This system treats language not as a sequence of tokens to be predicted, but as a **projection** of a precise point in a logical, semantic space.

## Architecture

The code implements a simplified version of the **Tri-Layer Cognitive Architecture**:

1.  **The Semantic Space (Layer 1 & 2 Proxy):** The `ExtendedSemanticSVOTransformer` class defines the core "mind" of the system.
    *   `self.verb_embeddings`: A dictionary mapping verbs to their base 10-dimensional semantic vectors. This represents the system's core knowledge of concepts.
    *   `self.axis_jacobians`: A dictionary mapping linguistic modifiers to their corresponding transformation vectors. This is the engine of logical manipulation.

2.  **The Sentence Assembler (Layer 3 - The "Mouth"):**
    *   The `SentenceAssembler` class takes the final, structured semantic data (the subject, the verb's final state, the object) and applies the complex, arbitrary rules of English grammar. It handles verb conjugation, auxiliary verbs (`do`, `be`, `will`), negation, and word order to produce the final, human-readable sentence.

## Key Features

*   **Whitebox & Interpretable:** Every step of the process is transparent. You can inspect the base vectors, the transformation vectors, and the final vector to understand exactly why the system produced a specific output.
*   **Zero-Shot Learning:** The system requires no training. Its logic is architected, not learned from data.
*   **Robust & Deterministic:** Given the same inputs, the system will always produce the exact same output. It is immune to the probabilistic "hallucinations" of large language models.
*   **Efficient:** The engine operates on small, 10-dimensional vectors and a clear set of rules, making it incredibly fast and computationally inexpensive.

## How to Run

This project requires Python and the `numpy` library.

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/your-repository-name.git
    cd your-repository-name
    ```

2.  **Install dependencies:**
    ```bash
    pip install numpy
    ```

3.  **Run the demo script:**
    ```bash
    python semantictenses08.py
    ```

## Example Walkthrough

Let's trace the generation of the sentence: `"She does not walk the dog"`

1.  **Input:** `subj='she'`, `verb='walk'`, `obj='the dog'`, `mods=['tense_present', 'polarity_neg']`
2.  **Initialization:** The system retrieves the base vector for `walk`: `[0.2, 0.0, 0.1, ...]`
3.  **Transformation:**
    *   It adds the vector for `tense_present` (`[0.0, 0.0, 0.0, ...]`), which doesn't change the vector.
    *   It adds the vector for `polarity_neg` (`[0.0, 0.0, 0.0, ..., -1.2, ...]`).
4.  **Final Semantic State:** The resulting vector is identical to the `walk` vector, except its 7th dimension (polarity) now has a strongly negative value.
5.  **Interpretation:** The system decodes this final vector:
    *   `tense` is 'present' (from the 2nd dimension's value).
    *   `polarity` is 'negative' (from the 7th dimension's value).
6.  **Assembly:** The `SentenceAssembler` receives these structured facts. It knows:
    *   The tense is present.
    *   The polarity is negative.
    *   There are no other auxiliaries (like modals).
    *   Therefore, it must perform **"do-support"**.
    *   The subject is 'she' (3rd person singular), so the correct form is 'does'.
    *   It assembles the final sentence: "She" + "does not" + "walk" + "the dog".

## The Broader Vision

This small script is a proof-of-concept for a new paradigm in artificial intelligence. It suggests that a path to more reliable, explainable, and efficient AI lies in separating the logic of meaning from the statistics of language. By building systems that operate on a clean, architected "semantic space," we can create machines that don't just mimic human language, but begin to reason in a way we can understand and trust.

